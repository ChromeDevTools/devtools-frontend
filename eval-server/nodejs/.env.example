# Evaluation Server Configuration
# Copy this file to .env and configure your settings

# Server Configuration
PORT=8080
HOST=127.0.0.1

# LLM Provider API Keys
# Configure one or more providers for evaluation

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key-here

# LiteLLM Configuration (if using a LiteLLM server)
LITELLM_ENDPOINT=http://localhost:4000
LITELLM_API_KEY=your-litellm-api-key-here

# Groq Configuration
GROQ_API_KEY=gsk_your-groq-api-key-here

# OpenRouter Configuration
OPENROUTER_API_KEY=sk-or-v1-your-openrouter-api-key-here

# Default LLM Configuration for Evaluations
# These will be used as fallbacks when not specified in evaluation requests
DEFAULT_PROVIDER=openai
DEFAULT_MAIN_MODEL=gpt-4
DEFAULT_MINI_MODEL=gpt-4-mini
DEFAULT_NANO_MODEL=gpt-3.5-turbo

# Logging Configuration
LOG_LEVEL=info
LOG_DIR=./logs

# Client Configuration
CLIENTS_DIR=./clients
EVALS_DIR=./evals

# RPC Configuration
RPC_TIMEOUT=30000

# Security
# Set this to enable authentication for client connections
# Leave empty to disable authentication
AUTH_SECRET_KEY=